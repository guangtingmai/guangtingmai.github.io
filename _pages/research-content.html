---
layout: archive
title: "Research"
permalink: /research-content/
author_profile: true
---
<p align="center">
  <img src="https://guangtingmai.github.io/images/brain_processes.png" width="100%">
</p>

<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.6;display: block;margin-bottom: 0.01vh">My interests focus on three brain processes as lenses to 
  understand typical and <i>atypical</i> human speech processing: <strong>(1) ‘neural tracking’</strong> (how brain signals align and respond to millisecond-scale acoustic 
  and linguistic features) of continuous speech; <strong>(2) functional connectivity</strong> between cortical (sensory and higher-order cognitive and 
  language-related) regions; and <strong>(3) inter-brain synchrony</strong> (brain-to-brain ‘communication’) during interpersonal interactions. 
  I am keen to understand how these processes are shaped/altered by sensory auditory or higher-level cognitive/language comprehension disorders, 
  and what roles they may play when the brain reorganises to compensate for these disorders. I am also keen to understand whether and how neural tracking
  may <i>causally</i> be related to (i.e., consequentially modulate, rather than be a by-product of) speech comprehension in clinical individuals. I combine neuroimaging methods of 
  <strong>electroencephalography</strong> (EEG) (measuring neural tracking) and <strong>functional near infrared spectroscopy</strong> (fNIRS) 
  (measuring functional connectivity and inter-brain synchrony).</span></p>
<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.6;display: block">I am parts of ongoing projects that look into these processes in aphasic or cochlear implanted individuals:</span>

<div class="flex-row">
  <div style="width: 28%; text-align: center;"><a href="https://guangtingmai.github.io/crossmodal-ci/">
    <img src="https://guangtingmai.github.io/images/cross-modal-ci-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Neural tracking of audiovisual speech in cochlear-implanted adults</strong></a> (with Dr <a href="https://www.mrc-cbu.cam.ac.uk/people/matt.davis/">Matt Davis</a>)
    </p>
  </div>
  <div style="width: 26%; text-align: center;"><a href="https://osf.io/preprints/psyarxiv/uqvtk_v1">
    <img src="https://guangtingmai.github.io/images/neural-tracking-aphasia-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Neural tracking of auditory speech in post-stroke aphasia</strong></a> (PI Dr <a href="https://profiles.ucl.ac.uk/82001-holly-robson">Holly Robson</a>)
    </p>
  </div>
  <div style="width: 29%; text-align: center;"><a href="https://guangtingmai.github.io/interbrain-sync/">
    <img src="https://guangtingmai.github.io/images/child-mother-brain-synchrony-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Inter-brain synchrony during mother-child interactions</strong></a> (PI Prof <a href="https://www.nottingham.ac.uk/Medicine/people/douglas.hartley">Doug Hartley</a>)
    </p>
  </div>
  
</div>
</p>

<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.6;display: block;margin-bottom: 0.05vh">Previously, I did studies that explore neural speech tracking and/or 
  plastic changes in speech-based brain functions in typical listening and age-related hearing loss:</span>

<div class="flex-row">
  <div style="width: 29%; text-align: center;"><a href="https://guangtingmai.github.io/neuraltrack-typical/">
    <img src="https://guangtingmai.github.io/images/neural-tracking-typical-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Neural tracking of auditory speech in typically listening adults</strong></a>
    </p>
  </div>
  <div style="width: 25%; text-align: center;"><a href="https://guangtingmai.github.io/neuraltrack-hearingloss/">
    <img src="https://guangtingmai.github.io/images/neural-tracking-hearing-loss-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Neural tracking of auditory speech in adults with age-related hearing loss</strong></a>
    </p>
  </div>
  <div style="width: 29%; text-align: center;"><a href="https://guangtingmai.github.io/neuraltrack-hearingloss/">
    <img src="https://guangtingmai.github.io/images/neuroplasticity-hearing-loss-icons.png" style="width:100%; display:block;">
    <p style="width:100%; margin:0.4em 0 0; font-size:10.5pt; font-family:Tahoma;text-align:center;">
      <strong>Neuroplasticity of auditory speech processing in adults with age-related hearing loss</strong></a>
    </p>
  </div>
</div>
</p>

<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.6;display: block;">Hopefully these pieces of work could advance understanding of brain processing of speech and 
  also contribute to developing useful clinical tools for objectively monitoring speech comprehension outcomes, esp when reliable behavioural measures are not easy 
  to obtain in many clinical individuals; for the long term, contribute to bases for neuroscience-inspired rehabilitation to support - not limited to hearing loss 
  or aphasia but wider - neuropsychological conditions (e.g., neurodivergence, neurodegeneration) when day-to-day speech comprehension and communication become 
  challenging.</span></p>
<br>
<div style="font-size: 11pt;font-family:Tahoma;line-height: 1.6;" class="navigation-row">
  <span><a href="https://guangtingmai.github.io/bio/"><< Previous (Bio)</a></span>
  <span><a href="https://guangtingmai.github.io/research-links/">Next (Links) >></a></span>
</div>
