---
layout: archive
title: "CrossModal-CI"
permalink: /crossmodal-ci/
author_profile: true
---
<p align="left">
  <img src="https://guangtingmai.github.io/images/Blausen_0244_CochlearImplant_01.png" width="40%">
  <br>
</p>
<p><span style="font-size: 12pt;line-height: 1.4;display: block;margin-bottom: 0.12vh">A cochlear implant (CI) is a neuro-prothesis that helps people with severe hearing loss to access sounds. CI transmits sound vibrations into electrical stimulation that (re)activates cochlea cells and sends neural signals to the auditory cortex. However, electrical stimulation is artificial which does not provide natural auditory input as in typically hearing individuals. It remains a huge challenge for CI recipients during speech communication in day-to-day complex, noisy listening environments (at school/work, in streets/restaurants/train stations, meeting with friends and families). CI recipients often use visual speech cues (e.g., lipreading) to compensate for these auditory barriers to help communications. However, mechanisms underlying how the CI brains integrate audio and visual speech information are still not clear. It also remains unclear how interactions of brain functions between modalities (audition and vision) change or reorganise over time to perceive audiovisual speech and how reorganisation predicts future comprehension outcomes.</span></p>
<p><span style="font-size: 12pt;line-height: 1.4;display: block;margin-bottom: 0.12vh">Our Wellcome Trust funded ECA fellowship project (2025-2031, fEC: £1.6m) promises to conduct a series of cross-sectional and longitudinal experiments in adult CI recipients. We will combine portable neuroimaging techniques of EEG and high-density fNIRS or diffuse optical tomography (DOT) to measure neural tracking and responses to audio and visual speech in CI recipients’ brains – how these responses differ from typical hearing, how they could change over time and be modulated to improve speech comprehension. We will thus, scientifically, showcase example processes of <a href="https://en.wikipedia.org/wiki/Cross_modal_plasticity">cross-modal neuroplasticity</a> (evolvement of neural interactions between modalities, i.e., audition and vision) for research in humans with sensory impairment after rehabilitation.
<p><span style="font-size: 12pt;line-height: 1.4;display: block;margin-bottom: 0.12vh">Wider implications, e.g., outcomes from multimodal imaging and clinical practicality (for not just CI recipients) may be expected. We may provide evidence for the power for monitoring and prognosis of speech and language comprehension outcomes using sophisticated and portable neuroimaging tools, e.g., EEG combined with fNIRS/DOT compared to tools commonly used but far less physically or financially practical/accessible for treatments for a wide range of clinical individuals (e.g., MRI/fMRI). This is particularly useful for many whose behavioural data are not easy to be reliably captured to reflect actual speech comprehension. We may also provide consequential neuromodulation evidence for future techniques researchers could develop to support or assist speech and language therapy, in the longer run, for people with not only hard-of-hearing but other conditions such as aphasia, developmental disorders, neurodivergence, or neurodegeneration.

