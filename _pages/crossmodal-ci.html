---
layout: archive
title: ""
permalink: /crossmodal-ci/
---
<p span style="font-size: 20pt;font-family:Tahoma;line-height:1;display:block"><strong>CrossModal-CI</strong></span></p>
<p><div class="white-frame">
<span style="font-size: 11pt;font-family: Tahoma;line-height: 1.5;display: block;">
  Our <a href="https://wellcome.org/research-funding/funding-portfolio/funded-grants/understanding-and-improving-neural-tracking">new study</a>, currently funded by
  <a href="https://wellcome.org/">Wellcome Trust</a>, is aiming to explore how the brains of adults with cochlear implants process audiovisual speech.</span></p>
<p align="center">
  <img src="https://guangtingmai.github.io/images/Blausen_0244_CochlearImplant_01.png" width="31%">
  <img src="https://guangtingmai.github.io/images/lipreading.jpg" width="36%">
  <br>
</p>
<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.5;display: block;margin-bottom: 0.35vh">A <a href="https://en.wikipedia.org/wiki/Cochlear_implant">
  cochlear implant</a> (CI) is a neuro-prothesis that has helped <a href="https://pubs.aip.org/asa/jel/article/2/7/077201/2844572/Celebrating-the-one-millionth-cochlear-implanta">
  over 1 million deaf people worldwide restore their hearing</a>. This is achieved by transmitting sound vibrations into electrical stimulation that (re)activates 
  cochlear cells to send neural impulses to the auditory cortex. However, electrical stimulation is artificial which does not convey sufficient auditory 
  inputs as in typical hearing. Speech communication remains a huge challenge for CI recipients in day-to-day complex, noisy listening environments (at 
  school/work, in streets/restaurants/train stations, meeting with families and friends). CI recipients often use visual speech cues (e.g., lip-reading) 
  to compensate for these auditory barriers to help with communications. However, mechanisms underlying how their different sensory (auditory and visual) 
  systems work together to integrate audiovisual speech are still unclear. It also remains unclear how <i>interactions</i> between these different systems change 
  or 'reorganise' over time and how reorganisation predicts future speech comprehension outcomes.</span></p>
<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.5;display: block;margin-bottom: 0.35vh">The project promises to conduct 
  a series of cross-sectional and longitudinal experiments in adult CI recipients. We will combine brain imaging
  of <a href="https://en.wikipedia.org/wiki/Electroencephalography">electroencephalography (EEG)</a> (real-time neural activity with 
  millisecond precision) and high-density fNIRS/<a href="https://www.cuh.nhs.uk/news/cutting-edge-baby-brain-scan-technology-is-world-first/">diffuse optical tomography (HD-DOT)</a> (recording 
  blood oxygenation level at specific brain regions) to measure CI recipients’ neural responses to audio and visual speech – how they may differ from typical hearing, 
  change over time and be modulated to improve comprehension overcomes. We will thus, scientifically, showcase example processes of 
  <a href="https://en.wikipedia.org/wiki/Cross_modal_plasticity">cross-modal neuroplasticity</a> (how sensory systems with different modalities, e.g., 
  audition and vision, change the way they interact with each other in the brain) in humans with sensory impairment or deprivation after rehabilitation.</span></p>
<p align="center">
  <img src="https://guangtingmai.github.io/images/audiovisual_speech.png" width="65%">
  <br>
</p>
<p><span style="font-size: 11pt;font-family: Tahoma;line-height: 1.5;display: block;margin-bottom: 0.35vh"><strong>Wider implications</strong>, e.g., outcomes from 
  multimodal imaging and clinical practicality (not just for CI recipients) may be expected. We may provide evidence for the potential power for monitoring 
  and prognosis of speech and language comprehension outcomes using tools like EEG combined with portable HD-DOT compared to techniques commonly used but less 
  physically or financially practical/accessible – e.g., MRI/fMRI, under many circumstances – low-noise requirement for auditory screening, physical/mental 
  challenges for children/patients (e.g., staying still and focused for a long period in an enclosed environment), MRI's incompatibility with implanted electronic/metallic 
  devices, public fiscal affordability of large-scale generic/longitudinal (f)MRI screening, requirement of portability, bedside monitoring, etc. This is particularly 
  useful given the insufficient prognostic power relying solely on behavioural measures and for many whose 
  <a href="https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2025.1616963/full"> behavioural data are not easy to be 
  reliably captured</a> to reflect actual speech comprehension. We may also provide rigorous, consequential neuromodulation evidence for future techniques 
  researchers could develop to support existing speech and language therapies; in the longer run, for not only people with hard-of-hearing but 
  also those with conditions like aphasia, developmental disorders, neurodivergence, or neurodegeneration.</span></p>
<p align="left"><img src="https://guangtingmai.github.io/images/diversity.png" width="35%"></div></p>

<p><div class="white-frame"><span style="font-size: 13pt;line-height: 1;display: block;margin-bottom: 0.14vh"><strong>Funders (2025-2031):</strong></span></p>
<p align="left">
  <a href="https://wellcome.org/"><img src="https://guangtingmai.github.io/images/wellcome trust.png" width="11%"></a>
  &emsp;&emsp;<a href="https://www.mrc-cbu.cam.ac.uk/"><img src="https://guangtingmai.github.io/images/mrc_cbu.png" width="42%"></a>
</div></p>
<p span style="font-size:11pt;font-family:Tahoma;line-height:1;display:block"><a href="https://guangtingmai.github.io/research-content/"><< Back to Research</a></span></p>

